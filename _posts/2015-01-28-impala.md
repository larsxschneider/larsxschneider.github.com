---
layout: post
title: "How to programmatically execute Impala queries on EMR and write the results to a CSV file?"
description: ""
category:
tags: [aws, emr, impala, csv]
---
{% include JB/setup %}

I recently had to analyze a big chunk of data and I decided to use [Impala](http://impala.io/) with [Amazon Web Services (AWS) Elastic MapReduce (EMR)](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-impala.html). EMR has a nice web UI to create a cluster that you can afterwards connect to and run your queries on. However, stepping through the UI takes a while and I had to recreate this cluster every day to avoid paying 24/7. Since I had great experience accessing AWS APIs using [boto](http://boto.readthedocs.org/en/latest/index.html), I wanted to script the cluster setup and my query execution. In addition I wanted to write the results of the queries to [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) files and store them on [S3](https://aws.amazon.com/s3/).

As I turned out this was a bit more complicated than I thought. That's why I will explain the steps in the remainder of this post.

1. Define your AWS credentials, S3 bucket and key path for your CSV result file.  
_Note:_ This is just an example. You should never store your AWS credentials in a source repository. Instead use environment variables or config files! 
<code data-gist-id="922b8aaf07bc9b71ff39" data-gist-hide-footer="true" data-gist-line="1-8"></code>

1. Establish a connection to this S3 bucket.
<code data-gist-id="922b8aaf07bc9b71ff39" data-gist-hide-footer="true" data-gist-line="10-11"></code>

1. Create a _query file_ according to [Impala SQL](http://www.cloudera.com/content/cloudera/en/documentation/cdh5/v5-1-x/Impala/Installing-and-Using-Impala/ciiu_langref.html) and upload it to S3.
<code data-gist-id="922b8aaf07bc9b71ff39" data-gist-hide-footer="true" data-gist-line="13-17"></code>

1. Create a _query runner_ script that executes the query on EMR and upload it to S3. The query file from the last step is passed as parameter ([line 22](https://gist.github.com/larsxschneider/922b8aaf07bc9b71ff39/68738c74742292dc4d0652b3855efdcb5c07ec60#file-run_impala_query-py-L22)) and downloaded from S3 to the local machine ([line 26](https://gist.github.com/larsxschneider/922b8aaf07bc9b71ff39/68738c74742292dc4d0652b3855efdcb5c07ec60#file-run_impala_query-py-L26)). Afterwards the query is executed ([line 27](https://gist.github.com/larsxschneider/922b8aaf07bc9b71ff39/68738c74742292dc4d0652b3855efdcb5c07ec60#file-run_impala_query-py-L27)) and the result is written to a CSV formatted file ([line 28-31](https://gist.github.com/larsxschneider/922b8aaf07bc9b71ff39/68738c74742292dc4d0652b3855efdcb5c07ec60#file-run_impala_query-py-L28-L31)). Finally, the result is uploaded to S3 ([line 32-34](https://gist.github.com/larsxschneider/922b8aaf07bc9b71ff39/68738c74742292dc4d0652b3855efdcb5c07ec60#file-run_impala_query-py-L32-L34)). To avoid an error if the file already exists, the file is deleted before hand ([line 32](https://gist.github.com/larsxschneider/922b8aaf07bc9b71ff39/68738c74742292dc4d0652b3855efdcb5c07ec60#file-run_impala_query-py-L32)).
<code data-gist-id="922b8aaf07bc9b71ff39" data-gist-hide-footer="true" data-gist-line="19-35"></code>

1. Generate an EMR step that executes the _query runner_ script with with the _query file_ as parameter.
<code data-gist-id="922b8aaf07bc9b71ff39" data-gist-hide-footer="true" data-gist-line="37-45"></code>

1. Connect to EMR.
<code data-gist-id="922b8aaf07bc9b71ff39" data-gist-hide-footer="true" data-gist-line="47-51"></code>

1. Start a new EMR cluster and execute the generated EMR step.
<code data-gist-id="922b8aaf07bc9b71ff39" data-gist-hide-footer="true" data-gist-line="53-75"></code>

You can find the entire script [here](https://gist.github.com/larsxschneider/922b8aaf07bc9b71ff39).

Suggestion, comments, critique? Please drop me a note. Feedback is always appreciated!

